package com.aig.sds.spark;

import java.io.IOException;
import java.util.ArrayList;
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
import java.util.HashMap;
import java.util.LinkedHashSet;
import java.util.List;
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
import java.util.Map;

import javax.xml.parsers.ParserConfigurationException;

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
//import org.apache.spark.examples.streaming.StreamingExamples;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.xml.sax.SAXException;

import scala.Tuple2;

import com.aig.security.devicetypes.DeviceType;
import com.aig.security.devicetypes.bluecoat.BlueCoat;
import com.aig.security.devicetypes.btdiamond.BTDiamond;
import com.aig.security.devicetypes.mcafee.McAfee;
import com.aig.security.devicetypes.paloalto.PaloAlto;
import com.aig.security.devicetypes.trendmicro.TrendMicro;
import com.aig.security.solr.SecurityColl;
import com.google.common.collect.Lists;

/**
 * Consumes messages from one or more topics in Kafka.
 *
 * Usage: JavaKafkaWordCount
 * <zkQuorum> <group> <topics> <numThreads> <zkQuorum> is a list of one or more
 * zookeeper servers that make quorum <group> is the name of kafka consumer
 * group <topics> is a list of one or more kafka topics to consume from
 * <numThreads> is the number of threads the kafka consumer should use
 */
class JavaSQLContextSingleton {
	  static private transient SQLContext instance = null;
	  static public SQLContext getInstance(SparkContext sparkContext) {
	    if (instance == null) {
	      instance = new SQLContext(sparkContext);
	    }
	    return instance;
	  }
	}
public final class SDSLogAnalyze {
	//private static final Pattern SPACE = Pattern.compile(" ");
	//private static final long serialVersionUID = 1L;
	private static final Logger log = LoggerFactory.getLogger(SDSLogAnalyze.class);
	// private OutputCollector outputCollector;
	private static Map<PARSER, DeviceType> parsers;
	// static Map<String, Object> matchesMap = null;
	public StringBuffer stringbuffer = new StringBuffer();

	static enum PARSER {
		BLUECOAT, PALOALTO, TRENDMICRO, BTDIAMOND, MCAFEE
	};

	private SDSLogAnalyze() {
		initParsers();
	}

	private static void initParsers() {
		parsers = new HashMap<SDSLogAnalyze.PARSER, DeviceType>();
		try {
			parsers.put(PARSER.BLUECOAT, BlueCoat.getInstance());
			parsers.put(PARSER.PALOALTO, PaloAlto.getInstance());
			parsers.put(PARSER.TRENDMICRO, TrendMicro.getInstance());
			parsers.put(PARSER.BTDIAMOND, BTDiamond.getInstance());
			parsers.put(PARSER.MCAFEE, McAfee.getInstance());
		} catch (ParserConfigurationException e) {
			log.error(e.getMessage(), e);
		} catch (SAXException e) {
			log.error(e.getMessage(), e);
		} catch (IOException e) {
			log.error(e.getMessage(), e);
		}
	}

	private static DeviceType findParser(String logline) {
		if (logline != null && logline.startsWith("[") && logline.indexOf("]") != -1) {
			logline = logline.trim();
			String parserType = logline.substring(1, logline.indexOf("]"));
			System.out.println("parserType>>>>" + parserType);
			if (parserType.equalsIgnoreCase("cacheflowelff")) {
				System.out.println("got parser");
				System.out.println(parsers.get(PARSER.BLUECOAT));
				return parsers.get(PARSER.BLUECOAT);
			} else if (parserType.equalsIgnoreCase("paloalto")) {
				return parsers.get(PARSER.PALOALTO);
			} else if (parserType.equalsIgnoreCase("aigtrendmicrods")) {
				return parsers.get(PARSER.TRENDMICRO);
			} else if (parserType.equalsIgnoreCase("rhlinux")) {
				return parsers.get(PARSER.BTDIAMOND);
			} else if (parserType.equalsIgnoreCase("ePolicy")) {
				return parsers.get(PARSER.MCAFEE);
			} else {
				log.error("Parser not Defined. Please Define New Parser::" + logline);
				return null;
			}
		} else {
			return null;
		}

	}

	public static void main(String[] args) {
		if (args.length < 4) {
			System.err.println("Usage: JavaKafkaWordCount <zkQuorum> <group> <topics> <numThreads> <hdfsfolder>");
			System.exit(1);
		}
final String foldername =args[4];
		// StreamingExamples.setStreamingLogLevels();
		SparkConf sparkConf = new SparkConf().setAppName("SDSLogAnalyze");
		// JavaSparkContext ctx = new JavaSparkContext(sparkConf);
		// Create the context with a 1 second batch size
		JavaSparkContext sc = new JavaSparkContext(sparkConf);
		
		JavaStreamingContext jssc = new JavaStreamingContext(sc, new Duration(2000));
		//final SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);
		int numThreads = Integer.parseInt(args[3]);
		Map<String, Integer> topicMap = new HashMap<String, Integer>();
		String[] topics = args[2].split(",");
		for (String topic : topics) {
			topicMap.put(topic, numThreads);
		}
System.out.println("args[2]>>>>>>>>>>>>>>>>>>>>>>>>"+args[2]);
		JavaPairReceiverInputDStream<String, String> messages = KafkaUtils.createStream(jssc, args[0], args[1],
				topicMap);

		initParsers();

		final LinkedHashSet<String> csvheader = new LinkedHashSet<String>();
		csvheader.add("id");
		csvheader.add("deviceType");
		csvheader.add("date");
		csvheader.add("time");
		csvheader.add("sourceAddress");
		csvheader.add("action");
		csvheader.add("resultCode");
		csvheader.add("sentBytes");
		csvheader.add("webMethod");
		csvheader.add("userName");
		csvheader.add("webHost");
		csvheader.add("contentType");
		csvheader.add("destPort");
		csvheader.add("receivedBytes");
		csvheader.add("destAddress");
		csvheader.add("netwrokService");
		csvheader.add("duration");
		csvheader.add("tranDestAddress");
		csvheader.add("hCode");
		csvheader.add("groupObject");
		csvheader.add("fld68");
		csvheader.add("disposition");
		csvheader.add("userAgent");
		csvheader.add("virusName");
		csvheader.add("service");
		csvheader.add("url");
		csvheader.add("webPage");
		csvheader.add("message");
		csvheader.add("hostName");
		csvheader.add("sourcePort");
		csvheader.add("counter1");
		csvheader.add("context");
		csvheader.add("destMac");
		csvheader.add("protocol");
		csvheader.add("sourceMac");
		csvheader.add("fileName");
		csvheader.add("hostIP");
		csvheader.add("destHost");
		csvheader.add("processingTime");
		csvheader.add("signatureName");
		csvheader.add("destHostName");
		csvheader.add("direction");
		csvheader.add("interfaceCode");
		csvheader.add("rule");
		csvheader.add("product");
		csvheader.add("messageCode");
		csvheader.add("ruleId");
		csvheader.add("version");
		csvheader.add("vendorCategory");
		csvheader.add("referenceId");
		csvheader.add("severity");
		csvheader.add("categroy");
		csvheader.add("rawData");
		csvheader.add("eventDesc");
		csvheader.add("signautreID");
		csvheader.add("eventComputer");
		csvheader.add("deviceHostName");
		csvheader.add("tty");
		csvheader.add("command");
		csvheader.add("domain");
		csvheader.add("timezone");
		csvheader.add("processId");

//		JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
//			/**
//			 * 
//			 */
//			private static final long serialVersionUID = 1L;
//
//			public String call(Tuple2<String, String> tuple2) {
//				String logline = tuple2._2();
//				System.out.println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
//				System.out.println(logline);
//				// String logline = tuple2.getStringByField("str");
//
//				String line = "";
//				try {
//					DeviceType deviceType = findParser(logline);
//					if (deviceType == null) {
//						System.out.println("deviceType is null");
//					} else {
//						SecurityColl parsedObject = deviceType.parse(logline, true);
//						Map<String, Object> matchesMap = parsedObject.getCollectionMap();
//
//						StringBuffer sb = new StringBuffer();
////						Date date = new Date();
////						DateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd");
////						DateFormat timeFormat = new SimpleDateFormat("HH:mm:ss");
//						for (String header : csvheader) {
//							if (matchesMap.containsKey(header)) {
////								if(header.equals("date")){
////									sb.append(",");
////									sb.append(dateFormat.format(date));
////								}else if(header.equals("time")){
////									sb.append(",");
////									sb.append(timeFormat.format(date));
////								}else{
//								sb.append(",");
//								sb.append(matchesMap.get(header));
//								//}
//
//							} else {
//								sb.append(",");
//							}
//						}
//
//						line = sb.toString().replaceFirst(",", "");
//					//	System.out.println("parsedObject>>>>" + parsedObject);
//
//					//	System.out.println("MAPObject>>>>" + matchesMap);
//					}
//				} catch (Exception e) {
//					log.error(e.getMessage(), e);
//				}
//				System.out.println("line>>" + line);
//				System.out.println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
//				return line;
//			}
//		});
		
		
		
		List<StructField> fields = new ArrayList<StructField>();
//		for (String fieldName: csvheader) {
//		  fields.add(DataTypes.createStructField(fieldName, DataTypes.StringType, true));
//		}
		
		  fields.add(DataTypes.createStructField("name", DataTypes.StringType, true));

		  fields.add(DataTypes.createStructField("AAA", DataTypes.StringType, true));

		  fields.add(DataTypes.createStructField("BBB", DataTypes.StringType, true));

		final StructType schema = DataTypes.createStructType(fields);
		System.out.println("schemaLLLschemaLLschema  "+schema);
		
		JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
			/**
			 * 
			 */
			private static final long serialVersionUID = 1L;

			public String call(Tuple2<String, String> tuple2) {
				String logline = tuple2._2();
				System.out.println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
				System.out.println(logline);
				// String logline = tuple2.getStringByField("str");

				String line = "";
				try {
					DeviceType deviceType = findParser(logline);
					if (deviceType == null) {
						System.out.println("deviceType is null");
					} else {
						SecurityColl parsedObject = deviceType.parse(logline, true);
						Map<String, Object> matchesMap = parsedObject.getCollectionMap();

						StringBuffer sb = new StringBuffer();
						for (String header : csvheader) {
							if (matchesMap.containsKey(header)) {
								sb.append(",");
								sb.append(matchesMap.get(header));

							} else {
								sb.append(",");
							}
						}
						line = sb.toString().replaceFirst(",", "");
					}
				} catch (Exception e) {
					log.error(e.getMessage(), e);
				}
				System.out.println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
				return line;
			}
		});
		
		
		
		lines.foreach(new Function<JavaRDD<String>, Void>() {
		private static final long serialVersionUID = 1L;

		public Void call(JavaRDD<String> rdd) {
			SQLContext sqlContext = JavaSQLContextSingleton.getInstance(rdd.context());

			
			JavaRDD<Row> rowRDD = rdd.map(
					  new Function<String, Row>() {
						private static final long serialVersionUID = 1L;

						public Row call(String record) throws Exception {
							
					      String[] fields = record.split(",");
					      System.out.println(fields[0]);
					      return RowFactory.create(fields[0], fields[1].trim(),fields[2]);
					    }
					  });
			System.out.println("INSIDE>>>>>");
			System.out.println(rowRDD.count());
			DataFrame peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema);
			//"hdfs://dev-gfat/EDMBD/ISAPUG/sql"+
			peopleDataFrame.write().format("parquet").save("/tmp/sql"+foldername+".parquet");
			return null;
		}
	});
		
//		JavaDStream<Row> rowRDD = lines.map(
//		  new Function<String, Row>() {
//			private static final long serialVersionUID = 1L;
//
//			public Row call(String record) throws Exception {
//		      String[] fields = record.split(",");
//		      System.out.println(fields[0]);
//		      return RowFactory.create(fields[0], fields[1].trim(),fields[2]);
//		    }
//		  });
//		System.out.println("FIRST>>>>>>>>");
//		rowRDD.print();
//		rowRDD.foreach(new Function<JavaRDD<Row>, Void>() {
//			
//			private static final long serialVersionUID = 1L;
//
//			public Void call(JavaRDD<Row> rdd) {
//				System.out.println("FIRST>>>>>>>>");
//				System.out.println(rdd.count());
//				
//				DataFrame peopleDataFrame = sqlContext.createDataFrame(rdd, schema);
//				rdd.saveAsTextFile("hdfs://dev-gfat/EDMBD/ISAPUG/"+foldername);
//				peopleDataFrame.write().format("parquet").save("hdfs://dev-gfat/EDMBD/ISAPUG/sql"+foldername+".parquet");
//		//		rdd.saveAsTextFile("hdfs://dev-gfat/EDMBD/ISAPUG/load");
//				return null;
//			}
//		});
		

		
		
//		DataFrame df = null ;//= sqlContext.read().format("json").load("examples/src/main/resources/people.json");
//		df.select("name", "age").write().format("parquet").save("namesAndAges.parquet");
		// Apply the schema to the RDD.
		

		
		

//		// System.out.println("matchesMap>>>>" + matchesMap);
//
//		// List<String> jsonData = Arrays
//		// .asList("{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}");
//		// JavaRDD<String> anotherPeopleRDD = ctx.parallelize(jsonData);
//		lines.print();
//
//		// JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String,
//		// String>() {
//		// public Iterable<String> call(String x) {
//		// return Lists.newArrayList(SPACE.split(x));
//		// }
//		// });
//
//		JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
//			/**
//			 * 
//			 */
//			private static final long serialVersionUID = 1L;
//
//			public Iterable<String> call(String x) {
//				return Lists.newArrayList(x);
//			}
//		});
//
//		System.out.println("*************************************");
//		words.print();
//
//		System.out.println("*************************************");
//
//		// JavaDStream<String> words = lines.flatMap(new
//		// FlatMapFunction<String, String>() {
//		// public Iterable<String> call(String x) {
//		// return Lists.newArrayList(SPACE.split(x));
//		// }
//		// });
//
//		JavaPairDStream<String, Integer> wordCounts = words.mapToPair(new PairFunction<String, String, Integer>() {
//			/**
//			 * 
//			 */
//			private static final long serialVersionUID = 1L;
//
//			public Tuple2<String, Integer> call(String s) {
//				return new Tuple2<String, Integer>(s, 1);
//			}
//		}).reduceByKey(new Function2<Integer, Integer, Integer>() {
//			/**
//			 * 
//			 */
//			private static final long serialVersionUID = 1L;
//
//			public Integer call(Integer i1, Integer i2) {
//				return i1 + i2;
//			}
//		});
//
//		wordCounts.print();
		jssc.start();
		jssc.awaitTermination();
	}
}